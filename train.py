import torch
import torch.nn as nn
import torch.optim as optim
import os
import time
import json

# å¯¼å…¥æˆ‘ä»¬å¯ä»¥è‡ªå®šä¹‰çš„æ¨¡å—
from models.resnet import resnet18
from utils.dataset import get_cifar10_loaders

# --------------------------------------------------------------------------
# è¶…å‚æ•°è®¾ç½® (æŒ‰ç…§ä»»åŠ¡ä¹¦è¦æ±‚)
# --------------------------------------------------------------------------
BATCH_SIZE = 128
LEARNING_RATE = 0.1
MOMENTUM = 0.9
WEIGHT_DECAY = 5e-4
EPOCHS = 30  # å»ºè®®è‡³å°‘è·‘ 20-30 è½®ï¼ŒResNet éœ€è¦å¤šä¸€ç‚¹æ—¶é—´æ”¶æ•›
MILESTONES = [15, 25] # åœ¨ç¬¬ 15 å’Œ 25 epoch é™ä½å­¦ä¹ ç‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SAVE_DIR = "./result/checkpoints"
LOG_DIR = "./result/logs"

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

def train_one_epoch(model, loader, criterion, optimizer, epoch):
    """
    è®­ç»ƒä¸€ä¸ª Epoch
    """
    model.train() # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ (å¯ç”¨ BN å’Œ Dropout)
    running_loss = 0.0
    correct = 0
    total = 0
    
    start_time = time.time()
    
    for i, (inputs, targets) in enumerate(loader):
        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
        
        # 1. æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()
        
        # 2. å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # 3. åå‘ä¼ æ’­
        loss.backward()
        
        # 4. æ›´æ–°å‚æ•°
        optimizer.step()
        
        # ç»Ÿè®¡æ•°æ®
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    end_time = time.time()
    
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    
    print(f"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}% | Time: {end_time-start_time:.1f}s")
    return epoch_loss, epoch_acc

def evaluate(model, loader, criterion):
    """
    éªŒè¯/æµ‹è¯•æ¨¡å‹
    """
    model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (é”å®š BN çŠ¶æ€, ç¦ç”¨ Dropout)
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad(): # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜
        for inputs, targets in loader:
            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
    avg_loss = test_loss / len(loader)
    acc = 100. * correct / total
    
    print(f"    >>> Test Loss: {avg_loss:.4f} | Test Acc: {acc:.2f}%")
    return avg_loss, acc

def main():
    print(f"Using Device: {DEVICE}")
    
    # 1. å‡†å¤‡æ•°æ®
    train_loader, val_loader, test_loader = get_cifar10_loaders(batch_size=BATCH_SIZE)
    
    # 2. æ„å»ºæ¨¡å‹
    model = resnet18(num_classes=10).to(DEVICE)
    
    # 3. å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    # 4. å®šä¹‰ä¼˜åŒ–å™¨ (SGD + Momentum)
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, 
                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
    
    # 5. å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ (MultiStepLR)
    # åœ¨æŒ‡å®šçš„ milestones èŠ‚ç‚¹å°†å­¦ä¹ ç‡ä¹˜ä»¥ 0.1
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=0.1)
    
    # è®°å½•è®­ç»ƒå†å²ï¼Œç”¨äºåç»­ç”»å›¾
    history = {
        "train_loss": [], "train_acc": [],
        "test_loss": [],  "test_acc": []
    }
    
    best_acc = 0.0
    
    print("å¼€å§‹è®­ç»ƒ...")
    start_global = time.time()
    
    for epoch in range(EPOCHS):
        # è®­ç»ƒä¸€è½®
        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, epoch)
        
        # éªŒè¯ä¸€è½®
        v_loss, v_acc = evaluate(model, test_loader, criterion)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        print(f"    Current LR: {current_lr}")

        # è®°å½•æ•°æ®
        history["train_loss"].append(t_loss)
        history["train_acc"].append(t_acc)
        history["test_loss"].append(v_loss)
        history["test_acc"].append(v_acc)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if v_acc > best_acc:
            print(f"    ğŸ‰ New Best Acc: {v_acc:.2f}% (Saved)")
            best_acc = v_acc
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'resnet18_cifar10_best.pth'))
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'resnet18_cifar10_last.pth'))
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿— (JSONæ ¼å¼ï¼Œæ–¹ä¾¿ visualize è¯»å–)
    with open(os.path.join(LOG_DIR, 'training_history.json'), 'w') as f:
        json.dump(history, f)
        
    print(f"\nè®­ç»ƒç»“æŸï¼æ€»è€—æ—¶: {(time.time() - start_global)/60:.1f} min")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"æ—¥å¿—å·²ä¿å­˜è‡³: {LOG_DIR}")

if __name__ == '__main__':
    main()